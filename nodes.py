

from langgraph.types import interrupt, Command
from langchain_core.messages import ToolMessage, AIMessage
from langchain_core.runnables import RunnableLambda
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import AnyMessage, add_messages
from agents import query_check_agent,query_gen_agent,query_executor_agent
from tools import get_schema_tool
from typing import Annotated, Literal
from llm_initialization import llm
from typing import Annotated,Optional
from typing_extensions import TypedDict


# Define the state for the agent
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    sql_query: Optional[str] = None


# used to handle an error thrown by a tool
def handle_tool_error(state) -> dict:
    error = state.get("error")
    tool_calls = state["messages"][-1].tool_calls
    return {
        "messages": [
            ToolMessage(
                content=f"Error: {repr(error)}\n please fix your mistakes.",
                tool_call_id=tc["id"],
            )
            for tc in tool_calls
        ]
    }


#toolnade with fallback to handle tool errors properly
def create_tool_node_with_fallback(tools: list):
    """
    Create a ToolNode with a fallback to handle errors and surface them to the agent.
    """
    return ToolNode(tools).with_fallbacks(
        [RunnableLambda(handle_tool_error)], exception_key="error"
    )





#  node for the first tool call to fetch the table names from the database
def first_tool_call(state: State) -> dict[str, list[AIMessage]]:
    return {
        "messages": [
            AIMessage(
                content="",
                tool_calls=[
                    {
                        "name": "sql_db_list_tables",
                        "args": {},
                        "id": "tool_abcd123",
                    }
                ],
            )
        ]
    }


# node for a model to fetchthe schema of the  relevant tables based on the question and available tables
model_get_schema = llm.bind_tools([get_schema_tool])



# node with agent to check a query sql query generated by the query generation agent
def model_check_query(state: State):
    print("Double checking query generated by gen node in correct query node\n")
    message=query_check_agent.invoke(state)
    print(message["messages"][-1].content)
    return {"messages": [message["messages"][-1].content], "sql_query": message["messages"][-1].content}





#node with the agent to generate a sql query based on the question asked by the user and the schema of relevannt tables
def query_gen_node(state: State):
    print("query generated by query gen node\n")
    message = query_gen_agent.invoke(state)
    print(message["messages"][-1].content)
    return {"messages": [message["messages"][-1].content], "sql_query": message["messages"][-1].content}



#node with agent to execute the generated and checked query after approval from the end user
def output_gen_node(state: State):
    print("output_gen_node")
    output=query_executor_agent.invoke(state)
    return {"messages": [output["messages"][-1].content], "sql_query": output["messages"][-1].content}


#node to handle the human in the loop mechanism
def human_approval(state: State) -> Command[Literal["output_gen_node", "query_gen"]]:
    
    is_approved = interrupt(
        {
            "question": "Is this correct?",
            "user_output": state["messages"][-1].content,
        }
    )
    
    #if the query is approved by then route to output gen node to execute the query
    if is_approved == "yes":
        return Command(goto="output_gen_node")
    else:
        #if feedback is given then go back to query generation node to revise the query
        revised_message = AIMessage(content=is_approved)
        return Command(goto="query_gen", update={"messages": [revised_message]})


